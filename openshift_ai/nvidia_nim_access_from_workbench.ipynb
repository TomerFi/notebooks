{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be3c4a9-1c93-44df-9e89-3113e0ffaa38",
   "metadata": {},
   "source": [
    "**OpenShift AI NVIDIA NIM**\n",
    "\n",
    "This Notebook demonstrates accessing an NIM model running internally in the same project as the Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d130fce-9ba9-481c-8483-7ea4b949c55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# After deploying an NIM model, grab the URL from the deployed model.\n",
    "nim_url = \"https://nim-deploy.nim-openshift-ai.svc.cluster.local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02d8d00-e8b9-461f-b635-a9b6c4c28d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"meta/llama3-8b-instruct\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1729805092,\n",
      "    \"owned_by\": \"system\",\n",
      "    \"root\": \"meta/llama3-8b-instruct\",\n",
      "    \"parent\": null,\n",
      "    \"permission\": [\n",
      "      {\n",
      "        \"id\": \"modelperm-937bf9a2ec6d475da69a8f04752b32b7\",\n",
      "        \"object\": \"model_permission\",\n",
      "        \"created\": 1729805092,\n",
      "        \"allow_create_engine\": false,\n",
      "        \"allow_sampling\": true,\n",
      "        \"allow_logprobs\": true,\n",
      "        \"allow_search_indices\": false,\n",
      "        \"allow_view\": true,\n",
      "        \"allow_fine_tuning\": false,\n",
      "        \"organization\": \"*\",\n",
      "        \"group\": null,\n",
      "        \"is_blocking\": false\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Fetch the available models, in our case meta/llama3-8b-instruct.\n",
    "response = requests.get(nim_url + \"/v1/models\", verify=False)\n",
    "print(json.dumps(response.json()['data'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fbd19a-5fba-4144-baa6-0bc9b227c0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think I can help you with that!\n",
      "\n",
      "NVIDIA NIM (NVIDIA Integrated Machine) is a cloud-based platform that enables data scientists and developers to easily deploy, manage, and scale AI and machine learning (ML) workloads across multiple cloud environments. NIM provides a unified platform for building, testing, and deploying AI and ML models, and it integrates with various NVIDIA technologies, such as NVIDIA GPU acceleration, NVIDIA Tensor Core processors, and NVIDIA DGX-1 systems.\n",
      "\n",
      "NIM offers a range of features, including:\n",
      "\n",
      "1. Model development and testing: NIM provides a cloud-based environment for data scientists to develop, test, and fine-tune their AI and ML models using popular frameworks like TensorFlow, PyTorch, and Caffe.\n",
      "2. Model deployment: NIM allows developers to deploy their trained models to various cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and on-premises data centers.\n",
      "3. Model management: NIM provides a centralized platform for managing and monitoring AI and ML models, including version control, model tracking, and performance monitoring.\n",
      "4. Collaboration: NIM enables collaboration among data scientists, developers, and other stakeholders through features like model sharing, version control, and real-time feedback.\n",
      "\n",
      "By using NIM, organizations can accelerate their AI and ML projects, improve collaboration, and reduce the complexity of deploying and managing AI workloads across multiple environments.\n",
      "\n",
      "Now, regarding Red Hat OpenShift AI, it's a cloud-native platform that enables organizations to deploy and manage AI and ML workloads on-premises or in the cloud. OpenShift AI is built on top of Red Hat OpenShift, a popular container orchestration platform, and provides a scalable and secure environment for building, deploying, and managing AI and ML applications.\n",
      "\n",
      "OpenShift AI integrates with various AI and ML frameworks, such as TensorFlow, PyTorch, and scikit-learn, and provides features like:\n",
      "\n",
      "1. Containerized AI and ML workloads: OpenShift AI enables data scientists and developers to package their AI and ML workloads into containers, making it easier to deploy, manage, and scale them.\n",
      "2. Scalable infrastructure: OpenShift AI provides a scalable infrastructure that can handle large-scale AI and ML workloads, with support for NVIDIA GPU acceleration and other specialized hardware.\n",
      "3. Security and compliance: OpenShift AI provides a secure environment for AI and ML workloads, with features like encryption, access controls, and compliance with industry regulations.\n",
      "4. Collaboration: OpenShift AI enables collaboration among data scientists, developers, and other stakeholders through features like version control, model tracking, and real-time feedback.\n",
      "\n",
      "By using OpenShift AI, organizations can accelerate their AI and ML projects, improve collaboration, and reduce the complexity of deploying and managing AI workloads.\n"
     ]
    }
   ],
   "source": [
    "# Chat with the target model.\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is Red Hat OpenShift AI?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is NVIDIA NIM?\"\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(nim_url + \"/v1/chat/completions\", verify=False, headers=headers, json=payload)\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
